# -*- coding: utf-8 -*-
"""Copy of FEC_Scheme_Recognition.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iwzklRq5kbkmJcH13QfeW3OwDDm-PDK-
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import tensorflow as tf
import sklearn
from keras.utils import to_categorical
from keras.models import save_model
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM,Dense,Dropout
from sklearn.preprocessing import LabelEncoder,StandardScaler
from keras.preprocessing.sequence import pad_sequences
import matplotlib.pyplot as plt
from sklearn.metrics import mean_absolute_error
from tensorflow.keras.layers import BatchNormalization

df=pd.read_csv('/content/drive/MyDrive/FEC_SCHEME DETECTION/dataset_2.csv',dtype={'Demodulated_Data': str,'FEC_scheme':str})
df=df.sample(frac=1)

df['Demodulated_Data'] = df.Demodulated_Data.apply(lambda x: str(x))
df['Demodulated_Data'] = df.Demodulated_Data.apply(lambda x: x.split(','))
df['Demodulated_Data'] = df.Demodulated_Data.apply(lambda x: list(map(float, x)))
df['Demodulated_Data'] = df.Demodulated_Data.apply(lambda x: np.asarray(x).astype('float32'))
df['Demodulated_Data'] = df.Demodulated_Data.apply(lambda x: np.pad(x, (1800-len(x),0), mode='constant'))

x = df['Demodulated_Data'].to_numpy()
y = df['FEC_Scheme']

x_normalized = []
for arr in x:
    arr_range = np.max(arr) - np.min(arr)
    if arr_range != 0:
        normalized_arr = (arr - np.min(arr)) / arr_range
        x_normalized.append(normalized_arr)
    else:
        x_normalized.append(arr)

x= np.array(x_normalized)

x_train, x_test, y_train, y_test = train_test_split(x, y,test_size=0.2)
x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))
x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], 1))
# X_train = np.asarray(X_train).astype(np.float32)
# y_train = np.asarray(y_train).astype(np.float32)
print("\nX_train:\n")

print(x_train.shape)

print("\nX_test:\n")
print(x_test.shape)

print("\ny_train:\n")
print(y_train.shape)

print("\ny_test:\n")
print(y_test.shape)

print()

print(x_train[0].shape)
print(y_train.shape)

model = Sequential()
model.add(LSTM(128 ,name = 'lstm_1', input_shape=(1800,1), return_sequences=True))
model.add(Dropout(0.2,name = 'dropout_1'))
model.add(LSTM(128,name = 'lstm_2'))
model.add(Dropout(0.2,name = 'dropout_2'))
model.add(BatchNormalization())
model.add(Dense(32,name = 'dense_1', activation='relu'))
model.add(BatchNormalization())
model.add(Dense(32,name = 'dense_2', activation='relu'))
model.add(BatchNormalization())
model.add(Dense(16,name = 'dense_3', activation='relu'))
model.add(BatchNormalization())
model.add(Dense(5,name = 'dense_4', activation='softmax'))
opt = tf.keras.optimizers.Adam(learning_rate=0.0001)
model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])

model.summary()

label_encoder = LabelEncoder()

y_train_encoded = label_encoder.fit_transform(y_train)
# y_train_one_hot = to_categorical(y_train_encoded, num_classes=4)

x_train = np.array([np.array(seq) for seq in x_train])
x_train_tensor = tf.convert_to_tensor(x_train, dtype=tf.float32)
scaler = StandardScaler()
x_test = np.array([np.array(seq) for seq in x_test])
# Fit on training data and transform both training and test data

y_test_encoded = label_encoder.transform(y_test)

x_test_tensor = tf.convert_to_tensor(x_test, dtype=tf.float32)
history=model.fit(x_train_tensor, y_train_encoded, epochs=3, batch_size=64, validation_data=(x_test_tensor,y_test_encoded))

print(history.history.keys())
# summarize history for accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

print(y_train_encoded)
print(y_train[:3])

from joblib import dump,load

save_model(model,'model.h5')

x = "0,0,1,1,1,1,1,1,0,0,1,1,0,0,1,1,0,0,0,1,1,1,0,0,0,0,0,0,0,0,1,1,0,0,0,0,1,1,1,1,1,1,1,1,0,0,1,1,1,1,0,0,0,0,0,0,1,1,1,1,0,0,0,0,1,1,1,1,1,0,0,0,1,1,1,0,0,0,0,0,1,1,0,0,1,0,0,0,0,0,1,1,0,0,1,1,0,0,0,0,1,1,1,1,0,1,0,0,1,1,1,1,0,0,1,1,0,0,0,0,0,0,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,1,1,0,0,1,0,1,1,0,0,1,1,1,0,0,1,1,1,0,1,1,1,1,1,1,1,0,1,0,1,1,1,0,1,0,1,1,0,0,1,0,0,1,1,0,0,0,0,1,1,1,1,0,0,1,1,0,0,1,1,0,0,0,0,1,0,0,0,1,0,0,1,1,0,0,0,1,1,1,0,0,0,0,0,0,0,1,1,0,0,1,1,0,0,0,0,0,0,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,1,1,0,0,1,1,0,0,1,1,0,0,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,1,1,1,1,1,1,0,0,1,1,1,1,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,0,1,1,1,1,0,0,1,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,1,1,1,0,0,1,1,0,0,0,0,0,0,0,0,1,0,1,1,0,0,0,1,1,1,0,0,1,1,0,0,0,0,1,1,0,0,1,1,1,1,1,1,0,1,0,0,1,1,0,0,0,0,0,0,1,1,0,0,1,1,1,1,0,0,0,0,1,1,0,0,1,1,1,1,0,0,1,0,0,0,1,1,1,1,0,0,1,1,1,1,1,1,1,1,0,0,1,1,1,1,0,0,1,1,1,1,0,1,0,0,1,1,0,0,0,0,0,0,1,1,1,1,1,1,1,1,0,0,1,1,0,0,1,1,1,1,0,0,1,1,0,0,0,0,0,0,0,0,0,0,1,0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,1,1,1,1,0,1,0,0,0,0,1,1,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,1,0,0,1,1,1,1,1,1,0,0,0,0,1,1,1,1,1,1,1,1,0,0,1,1,0,0,0,1,0,0,0,0,1,1,0,0,0,0,1,1,0,0,0,0,1,1,1,1,1,1,1,1,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,1,1,0,0,1,1,1,1,1,1,0,0,0,0,1,0,1,1,0,0,1,1,1,1,0,0,0,1,1,1,1,1,0,0,0,1,1,1,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,1,1,1,0,0,1,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,0,0,1,1,1,1,0,0,1,1,0,0,1,1,0,0,0,0,0,0,1,1,0,0,1,1,1,1,1,1,1,1,1,1,0,0,1,1,1,1,1,1,0,0,1,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,1,1,1,1,1,0,0,1,1,0,0,1,1,1,1,0,0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,1,1,1,1,1,1,1,1,1,1,0,0,1,1,0,0,0,0,1,1,0,0,1,1,1,1,1,1,0,0,1,1,0,0,0,1,0,0,1,1,1,1,0,0,1,1,0,0,0,0,0,0,0,0,0,0,1,1,0,0,1,1,0,0,0,0,1,1,0,0,0,0,1,1,1,0,1,1,1,1,1,1,0,0,0,1,1,1,1,0,1,1,1,1,1,1,0,0,1,1,1,0,0,0,1,1,0,0,1,1,1,1,1,1,0,0,0,0,1,1,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,1,1,0,0,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,1,0,1,1,0,0,0,0,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,0,0,1,1"
x = x.split(',')
x = [float(i) for i in x]
x = [np.asarray(i).astype('float32') for i in x]
x = (x - np.min(x)) / (np.max(x) - np.min(x))
x = np.pad(x, (0, 1800 - len(x)), mode='constant')
print(max(x))
a = model.predict(np.array([x]))
print(a)
print(np.argmax(a))

predicted_label = label_encoder.inverse_transform([np.argmax(a)])
print(predicted_label)

for original_label, encoded_label in zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)):
    print(f"{original_label} -> {encoded_label}")